{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.tree import DecisionTreeRegressor, export_graphviz\n",
        "import graphviz\n",
        "from IPython.display import Image\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import RidgeCV, Ridge\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import RidgeCV, Ridge\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import mean_absolute_error, classification_report, confusion_matrix\n",
        "from sklearn.decomposition import PCA\n",
        "from yellowbrick.regressor import ResidualsPlot\n",
        "\n",
        "\n",
        "path = '2010-23-data_sa_crime.csv'\n",
        "original_data = pd.read_csv(path)\n",
        "data = original_data.copy()\n",
        "\n",
        "data.head()\n",
        "\n",
        "\n",
        "columns_to_plot = [\"Offence count\", \"Taken to remand\", \"Postcode - Incident\"]\n",
        "sns.pairplot(data=data[columns_to_plot])\n",
        "\n",
        "category_column = \"Offence Level 1 Description\"\n",
        "columns_to_plot = [\"Offence count\", \"Taken to remand\"]\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.boxplot(x=category_column, y=\"Offence count\", data=data)\n",
        "plt.title(\"Box Plot of Offence count by Offence Level 1 Description\")\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(data[\"Offence count\"], bins=20, edgecolor='k')\n",
        "plt.xlabel(\"Offence count\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Histogram of Offence count\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(data[\"Taken to remand\"], bins=20, edgecolor='k')\n",
        "plt.xlabel(\"Taken to remand\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Histogram of Taken to remand\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(data[\"Offence count\"], data[\"Taken to remand\"], alpha=0.5)\n",
        "plt.xlabel(\"Offence count\")\n",
        "plt.ylabel(\"Taken to remand\")\n",
        "plt.title(\"Scatter Plot of Offence count vs. Taken to remand\")\n",
        "plt.show()\n",
        "\n",
        "#***A bar plot can show the average \"Offence count\" and \"Taken to remand\" for different values in a categorical column like \"Offence Level 2 Description.\"***\n",
        "category_column = \"Offence Level 2 Description\"\n",
        "columns_to_plot = [\"Offence count\", \"Taken to remand\"]\n",
        "mean_values = data.groupby(category_column)[columns_to_plot].mean()\n",
        "mean_values.plot(kind=\"bar\", figsize=(12, 6))\n",
        "plt.xlabel(category_column)\n",
        "plt.ylabel(\"Mean Value\")\n",
        "plt.title(\"Mean Offence count and Taken to remand by \" + category_column)\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()\n",
        "\n",
        "#***Histogram of 'Offence count'***\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "sns.histplot(data['Offence count'], bins=20, kde=True)\n",
        "plt.title('Histogram of Offence Count')\n",
        "plt.xlabel('Offence Count')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "#Box Plot of offence\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "sns.boxplot(data['Offence count'])\n",
        "plt.title('Box Plot of Offence Count')\n",
        "\n",
        "# Step 2: Check for Missing Values\n",
        "missing_values = data.isnull().sum()\n",
        "print(\"Missing Values:\")\n",
        "print(missing_values)\n",
        "\n",
        "sns.pairplot(data[['Reported Date', 'Offence count', 'Postcode - Incident']], diag_kind='kde')\n",
        "\n",
        "data.dropna(inplace=True)\n",
        "data = pd.get_dummies(data, columns=['Suburb - Incident', 'Offence Level 1 Description', 'Offence Level 2 Description', 'Offence Level 3 Description'])\n",
        "numeric_columns = ['Postcode - Incident', 'Offence count']\n",
        "\n",
        "\n",
        "\n",
        "print(data.columns)\n",
        "\n",
        "\n",
        "offence_level1_columns = [col for col in data.columns if 'Offence Level 1 Description' in col]\n",
        "offence_level1_counts = data[offence_level1_columns].sum()\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=offence_level1_counts.values, y=offence_level1_counts.index, palette='viridis')\n",
        "plt.title('Count of Offence Level 1 Description')\n",
        "plt.xlabel('Count')\n",
        "plt.ylabel('Offence Level 1 Description')\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "for column in numeric_columns:\n",
        "    if not pd.api.types.is_numeric_dtype(data[column]):\n",
        "        # Handle non-numeric values\n",
        "        data[column] = pd.to_numeric(data[column], errors='coerce')\n",
        "data.dropna(subset=numeric_columns, inplace=True)\n",
        "\n",
        "\n",
        "offence_level2_columns = [col for col in data.columns if 'Offence Level 2 Description' in col]\n",
        "offence_level2_counts = data[offence_level2_columns].sum()\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=offence_level2_counts.values, y=offence_level2_counts.index, palette='magma')\n",
        "plt.title('Count of Offence Level 2 Description')\n",
        "plt.xlabel('Count')\n",
        "plt.ylabel('Offence Level 2 Description')\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "data['Reported Date'] = pd.to_datetime(data['Reported Date'], format='%d/%m/%Y')\n",
        "data.dropna(inplace=True)\n",
        "\n",
        "\n",
        "data = original_data.copy()\n",
        "pos = data[data['Offence Level 1 Description'] == 1]\n",
        "\n",
        "pos_percentage = len(pos) / len(data)\n",
        "neg_percentage = 1 - pos_percentage\n",
        "\n",
        "print('positive instance percentage is', pos_percentage)\n",
        "print('negative instance percentage is', neg_percentage)\n",
        "\n",
        "\n",
        "#Violin plot of 'Offence count' vs. 'Offence Level 1 Description'\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.violinplot(data=data, x='Offence Level 1 Description', y='Offence count', palette='Set1')\n",
        "plt.title('Violin Plot of Offence Count vs. Offence Level 1 Description')\n",
        "plt.xticks(rotation=90)\n",
        "\n",
        "\n",
        "data['Reported Date'] = pd.to_datetime(data['Reported Date'], format='%d/%m/%Y')\n",
        "Y = data['Offence count']\n",
        "\n",
        "X = data.drop(['Reported Date', 'Offence count'], axis=1)\n",
        "X = pd.get_dummies(X)\n",
        "clf = DecisionTreeRegressor(max_depth=3)\n",
        "scores = cross_val_score(clf, X, Y, cv=5, scoring='r2')\n",
        "print('Mean R-squared:', scores.mean())\n",
        "\n",
        "clf.fit(X, Y)\n",
        "feature_names = list(X.columns)\n",
        "dot_data = export_graphviz(clf, out_file=None,\n",
        "                           feature_names=feature_names,\n",
        "                           filled=True, rounded=True,\n",
        "                           special_characters=True)\n",
        "\n",
        "graph = graphviz.Source(dot_data)\n",
        "graph.render(\"decision_tree\")\n",
        "Image(graph.render(\"decision_tree\", format='png'))\n",
        "\n",
        "\n",
        "#Time series plot of 'Reported Date' vs. 'Offence count'\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.lineplot(data=data, x='Reported Date', y='Offence count')\n",
        "plt.title('Time Series Plot of Offence Count')\n",
        "plt.xlabel('Reported Date')\n",
        "plt.ylabel('Offence Count')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.countplot(data=data, x='Offence Level 1 Description', palette='viridis')\n",
        "plt.title('Count of Offence Level 1 Description')\n",
        "plt.xlabel('Offence Level 1 Description')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=90)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.countplot(data=data, x='Offence Level 2 Description', palette='magma')\n",
        "plt.title('Count of Offence Level 2 Description')\n",
        "plt.xlabel('Offence Level 2 Description')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=90)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(data=data, x='Postcode - Incident', y='Offence count', palette='coolwarm')\n",
        "plt.title('Mean Offence count by Postcode - Incident')\n",
        "plt.xlabel('Postcode - Incident')\n",
        "plt.ylabel('Mean Offence count')\n",
        "plt.xticks(rotation=90)\n",
        "\n",
        "\n",
        "\n",
        "#Gausian NB\n",
        "sample_fraction = 0.1\n",
        "sampled_data = data.sample(frac=sample_fraction, random_state=42)\n",
        "Y = sampled_data['Offence count']\n",
        "X = sampled_data.drop(['Reported Date', 'Offence count'], axis=1)\n",
        "X = pd.get_dummies(X)\n",
        "\n",
        "k = 50\n",
        "selector = SelectKBest(chi2, k=k)\n",
        "X_selected = selector.fit_transform(X, Y)\n",
        "\n",
        "# Initialize empty lists for storing results\n",
        "y = []\n",
        "x = []\n",
        "\n",
        "for i in range(1, 16):\n",
        "    # Create and train a Gaussian Naive Bayes classifier\n",
        "    clf = GaussianNB()\n",
        "    scores = cross_val_score(clf, X_selected, Y, cv=10, scoring='accuracy')\n",
        "\n",
        "    y.append(np.array(scores).mean())\n",
        "    x.append(i)\n",
        "plt.plot(x, y)\n",
        "plt.xlabel('Max Depth')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Gaussian Naive Bayes Classifier Accuracy vs. Max Depth')\n",
        "plt.show()\n",
        "\n",
        "print('Accuracy Scores:', y)\n",
        "\n",
        "\n",
        "sample_fraction = 0.1\n",
        "sampled_data = data.sample(frac=sample_fraction, random_state=42)\n",
        "Y = sampled_data['Offence count']\n",
        "X = sampled_data[['Taken to remand']]\n",
        "y = []\n",
        "\n",
        "# Create and train a Gaussian Naive Bayes classifier with cross-validation\n",
        "clf = GaussianNB()\n",
        "scores = cross_val_score(clf, X, Y, cv=10, scoring='accuracy')\n",
        "mean_accuracy = np.array(scores).mean()\n",
        "print(f'Mean Accuracy: {mean_accuracy}')\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "clf.fit(X_train, Y_train)\n",
        "Y_pred = clf.predict(X_test)\n",
        "\n",
        "# Generate a classification report\n",
        "class_report = classification_report(Y_test, Y_pred)\n",
        "print(\"Classification Report:\\n\", class_report)\n",
        "conf_matrix = confusion_matrix(Y_test, Y_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", linewidths=.5)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "#Pie chart of class distribution\n",
        "\n",
        "class_distribution = Y.value_counts()\n",
        "labels = class_distribution.index\n",
        "sizes = class_distribution.values\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=140)\n",
        "plt.axis('equal')\n",
        "plt.title('Class Distribution')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#Linear SVC\n",
        "sample_fraction = 0.05\n",
        "sampled_data = data.sample(frac=sample_fraction, random_state=42)\n",
        "Y = sampled_data['Offence count']\n",
        "X = sampled_data.drop(['Reported Date', 'Offence count'], axis=1)\n",
        "X = pd.get_dummies(X)\n",
        "y = []\n",
        "x = []\n",
        "\n",
        "for i in range(1, 16):\n",
        "    # Create and train a Linear Support Vector Classifier (LinearSVC)\n",
        "    clf = LinearSVC(max_iter=1000)\n",
        "    scores = cross_val_score(clf, X, Y, cv=5, scoring='accuracy')\n",
        "    y.append(np.array(scores).mean())\n",
        "    x.append(i)\n",
        "\n",
        "plt.plot(x, y)\n",
        "plt.xlabel('Max Depth')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Linear Support Vector Classifier Accuracy vs. Max Depth')\n",
        "plt.show()\n",
        "\n",
        "print('Accuracy Scores:', y)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(x, y, marker='o', color='skyblue')\n",
        "plt.xlabel('Max Depth')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Scatter Plot of Max Depth vs. Accuracy')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(y, bins=10, color='skyblue', edgecolor='black')\n",
        "plt.xlabel('Accuracy')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Histogram of Accuracy')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#RIDGECV\n",
        "\n",
        "sample_fraction = 0.05\n",
        "sampled_data = data.sample(frac=sample_fraction, random_state=42)\n",
        "Y = sampled_data['Offence count']\n",
        "X = sampled_data.drop(['Reported Date', 'Offence count'], axis=1)\n",
        "X = pd.get_dummies(X)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Initialize a RidgeCV model with alpha values\n",
        "alphas = [0.01, 0.1, 1, 10, 100]\n",
        "clf = RidgeCV(alphas=alphas, store_cv_values=True)\n",
        "\n",
        "# Fit the RidgeCV model\n",
        "clf.fit(X_scaled, Y)\n",
        "optimal_alpha = clf.alpha_\n",
        "y = []\n",
        "x = []\n",
        "\n",
        "for i in range(1, 16):\n",
        "\n",
        "    clf = Ridge(alpha=optimal_alpha)\n",
        "\n",
        "    scores = cross_val_score(clf, X_scaled, Y, cv=5, scoring='r2', n_jobs=-1)\n",
        "\n",
        "    y.append(np.array(scores).mean())\n",
        "    x.append(i)\n",
        "plt.plot(x, y)\n",
        "plt.xlabel('Max Depth')\n",
        "plt.ylabel('R-squared')\n",
        "plt.title('Ridge Regression R-squared vs. Max Depth')\n",
        "plt.show()\n",
        "\n",
        "print('Optimal Alpha:', optimal_alpha)\n",
        "print('R-squared Scores:', y)\n",
        "\n",
        "\n",
        "# Line Plot of R-squared vs. Max Depth\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(x, y, marker='o', linestyle='-', color='b')\n",
        "plt.xlabel('Max Depth')\n",
        "plt.ylabel('R-squared')\n",
        "plt.title('Ridge Regression R-squared vs. Max Depth')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#Ridge Regression\n",
        "\n",
        "sample_fraction = 0.1\n",
        "sampled_data = data.sample(frac=sample_fraction, random_state=42)\n",
        "Y = sampled_data['Offence count']\n",
        "X = sampled_data.drop(['Reported Date', 'Offence count'], axis=1)\n",
        "X = pd.get_dummies(X)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Initialize empty lists for storing results\n",
        "y = []\n",
        "alphas = [0.1, 1.0, 10.0]\n",
        "\n",
        "for alpha in alphas:\n",
        "\n",
        "    clf = Ridge(alpha=alpha)\n",
        "\n",
        "    scores = cross_val_score(clf, X_scaled, Y, cv=5, scoring='r2', n_jobs=-1)  # Utilize all CPU cores and fewer folds\n",
        "\n",
        "    y.append(np.array(scores).mean())\n",
        "plt.plot(alphas, y)\n",
        "plt.xlabel('Alpha (Regularization Strength)')\n",
        "plt.ylabel('R-squared')\n",
        "plt.title('Ridge Regression R-squared vs. Alpha')\n",
        "plt.show()\n",
        "\n",
        "print('Best Alpha:', alphas[np.argmax(y)])\n",
        "print('Best R-squared Score:', max(y))\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from yellowbrick.regressor import ResidualsPlot\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit the model on the training data\n",
        "clf.fit(X_train, Y_train)\n",
        "visualizer = ResidualsPlot(clf)\n",
        "visualizer.fit(X_train, Y_train)\n",
        "visualizer.score(X_test, Y_test)\n",
        "visualizer.show()\n",
        "\n",
        "\n",
        "\n",
        "#PCA and KNN\n",
        "\n",
        "sample_fraction = 0.1\n",
        "sampled_data = data.sample(frac=sample_fraction, random_state=42)\n",
        "selected_features = ['Offence count', 'Taken to remand']\n",
        "X = sampled_data[selected_features]\n",
        "Y = X['Offence count']\n",
        "X = X.drop(['Offence count'], axis=1)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply PCA for dimensionality reduction\n",
        "n_components = min(X_scaled.shape[0], X_scaled.shape[1])\n",
        "pca = PCA(n_components=n_components)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_pca, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train a K-NN regressor\n",
        "knn = KNeighborsRegressor(n_neighbors=5)\n",
        "knn.fit(X_train, Y_train)\n",
        "Y_pred = knn.predict(X_test)\n",
        "\n",
        "# Calculate and print evaluation metrics\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "mae = mean_absolute_error(Y_test, Y_pred)\n",
        "print(f'Mean Absolute Error: {mae}')\n",
        "plt.scatter(Y_test, Y_pred)\n",
        "plt.xlabel('Actual Offence count')\n",
        "plt.ylabel('Predicted Offence count')\n",
        "plt.title('Actual vs. Predicted Offence count')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "residuals = Y_test - Y_pred\n",
        "plt.hist(residuals, bins=20)\n",
        "plt.xlabel('Residuals')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Histogram of Residuals')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "plt.scatter(Y_pred, residuals)\n",
        "plt.xlabel('Predicted Offence count')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Residuals vs. Predicted Offence count')\n",
        "plt.axhline(y=0, color='r', linestyle='--')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "plt.hist(Y, bins=20)\n",
        "plt.xlabel('Offence count')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Offence Count')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from yellowbrick.regressor import ResidualsPlot\n",
        "\n",
        "# Initialize and train a K-NN regressor\n",
        "knn = KNeighborsRegressor(n_neighbors=5)\n",
        "knn.fit(X_train, Y_train)\n",
        "Y_pred = knn.predict(X_test)\n",
        "visualizer = ResidualsPlot(knn)\n",
        "visualizer.fit(X_train, Y_train)\n",
        "visualizer.score(X_test, Y_test)\n",
        "visualizer.show()\n",
        "\n",
        "\n",
        "\n",
        "# Principal Component Analysis (PCA) Scatter Plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.scatter(X_pca[:, 0], Y, c=Y, cmap='viridis')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Offence count')\n",
        "plt.title('PCA Scatter Plot')\n",
        "plt.colorbar(label='Offence count')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Apply PCA for dimensionality reduction\n",
        "n_components = min(X_scaled.shape[0], X_scaled.shape[1])\n",
        "pca = PCA(n_components=n_components)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Explained Variance Ratio\n",
        "explained_variance_ratio = pca.explained_variance_ratio_\n",
        "cumulative_explained_variance = explained_variance_ratio.cumsum()\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(range(1, n_components + 1), explained_variance_ratio, alpha=0.5, align='center', label='Explained Variance Ratio')\n",
        "plt.step(range(1, n_components + 1), cumulative_explained_variance, where='mid', label='Cumulative Explained Variance')\n",
        "plt.xlabel('Principal Components')\n",
        "plt.ylabel('Explained Variance Ratio / Cumulative Explained Variance')\n",
        "plt.title('Scree Plot for PCA')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "#Models comparison\n",
        "\n",
        "sample_fraction = 0.1\n",
        "sampled_data = data.sample(frac=sample_fraction, random_state=42)\n",
        "Y_regression = sampled_data['Offence count']\n",
        "X_regression = sampled_data.drop(['Reported Date', 'Offence count'], axis=1)\n",
        "X_regression = pd.get_dummies(X_regression)\n",
        "\n",
        "# Define your target variable and features for classification\n",
        "Y_classification = sampled_data['Taken to remand']\n",
        "X_classification = sampled_data.drop(['Reported Date', 'Taken to remand'], axis=1)\n",
        "X_classification = pd.get_dummies(X_classification)\n",
        "regression_results = {}\n",
        "classification_results = {}\n",
        "\n",
        "# Standardize features for regression\n",
        "scaler_regression = StandardScaler()\n",
        "X_scaled_regression = scaler_regression.fit_transform(X_regression)\n",
        "\n",
        "# Standardize features for classification\n",
        "scaler_classification = StandardScaler()\n",
        "X_scaled_classification = scaler_classification.fit_transform(X_classification)\n",
        "\n",
        "# Perform PCA for regression\n",
        "n_components_regression = min(X_scaled_regression.shape[0], X_scaled_regression.shape[1])\n",
        "pca_regression = PCA(n_components=n_components_regression)\n",
        "X_pca_regression = pca_regression.fit_transform(X_scaled_regression)\n",
        "\n",
        "# Split the data into training and testing sets for regression\n",
        "X_train_regression, X_test_regression, Y_train_regression, Y_test_regression = train_test_split(\n",
        "    X_pca_regression, Y_regression, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Perform PCA for classification\n",
        "n_components_classification = min(X_scaled_classification.shape[0], X_scaled_classification.shape[1])\n",
        "pca_classification = PCA(n_components=n_components_classification)\n",
        "X_pca_classification = pca_classification.fit_transform(X_scaled_classification)\n",
        "\n",
        "# Split the data into training and testing sets for classification\n",
        "X_train_classification, X_test_classification, Y_train_classification, Y_test_classification = train_test_split(\n",
        "    X_pca_classification, Y_classification, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Models for Regression\n",
        "regression_models = {\n",
        "    'Ridge Regression': RidgeCV(alphas=[0.01, 0.1, 1, 10, 100], store_cv_values=True),\n",
        "    'K-NN Regressor': KNeighborsRegressor(n_neighbors=5),\n",
        "}\n",
        "\n",
        "# Models for Classification\n",
        "classification_models = {\n",
        "    'Random Forest Classifier': RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    'Support Vector Classifier': SVC(kernel='linear'),\n",
        "    'Gaussian Naive Bayes': GaussianNB(),\n",
        "}\n",
        "\n",
        "# Train and evaluate regression models\n",
        "for model_name, model in regression_models.items():\n",
        "    model.fit(X_train_regression, Y_train_regression)\n",
        "    Y_pred_regression = model.predict(X_test_regression)\n",
        "    mae = mean_absolute_error(Y_test_regression, Y_pred_regression)\n",
        "    regression_results[model_name] = mae\n",
        "\n",
        "# Train and evaluate classification models\n",
        "for model_name, model in classification_models.items():\n",
        "    model.fit(X_train_classification, Y_train_classification)\n",
        "    Y_pred_classification = model.predict(X_test_classification)\n",
        "    class_report = classification_report(Y_test_classification, Y_pred_classification)\n",
        "    conf_matrix = confusion_matrix(Y_test_classification, Y_pred_classification)\n",
        "    classification_results[model_name] = {'classification_report': class_report, 'confusion_matrix': conf_matrix}\n",
        "\n",
        "# Visualization 1: Compare MAE of Regression Models\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(regression_results.keys(), regression_results.values())\n",
        "plt.xlabel('Regression Models')\n",
        "plt.ylabel('Mean Absolute Error (MAE)')\n",
        "plt.title('Comparison of Regression Models')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "# Visualization 2: Classification Report for Classification Models\n",
        "for model_name, results in classification_results.items():\n",
        "    print(f\"Classification Report for {model_name}:\\n{results['classification_report']}\")\n",
        "\n",
        "# Visualization 3: Confusion Matrix Heatmap for Classification Models\n",
        "plt.figure(figsize=(12, 8))\n",
        "for i, (model_name, results) in enumerate(classification_results.items(), start=1):\n",
        "    plt.subplot(2, 2, i)\n",
        "    sns.heatmap(results['confusion_matrix'], annot=True, fmt=\"d\", cmap=\"Blues\", linewidths=.5)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.title(f'Confusion Matrix for {model_name}')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Visualization 4: Residual Plots for Ridge Regression\n",
        "ridge_regression = regression_models['Ridge Regression']\n",
        "visualizer = ResidualsPlot(ridge_regression)\n",
        "visualizer.fit(X_train_regression, Y_train_regression)\n",
        "visualizer.score(X_test_regression, Y_test_regression)\n",
        "visualizer.show()\n",
        "\n",
        "# Visualization 5: PCA Scatter Plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.scatter(X_pca_regression[:, 0], Y_regression, c=Y_regression, cmap='viridis')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Offence count')\n",
        "plt.title('PCA Scatter Plot')\n",
        "plt.colorbar(label='Offence count')\n",
        "plt.show()\n",
        "\n",
        "# Visualization 6: Principal Component Loadings Plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(range(1, n_components_regression + 1), pca_regression.components_[0], label='Principal Component 1')\n",
        "plt.xlabel('Original Features')\n",
        "plt.ylabel('Principal Component Loadings')\n",
        "plt.title('Principal Component Loadings Plot')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8t1dAHfagWSL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}